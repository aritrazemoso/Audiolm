<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Enhanced Audio Recording Interface</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        .spinner-border {
            display: none;
            margin-left: 8px;
            vertical-align: middle;
        }

        .loading .spinner-border {
            display: inline-block;
        }

        .loading #generateAudio {
            pointer-events: none;
            opacity: 0.65;
        }

        .recording-indicator {
            width: 12px;
            height: 12px;
            border-radius: 50%;
            background-color: #dc3545;
            display: inline-block;
            margin-right: 8px;
        }

        .recording .recording-indicator {
            animation: pulse 1.5s infinite;
        }

        .visualizer-container {
            height: 100px;
            background-color: #f8f9fa;
            border-radius: 8px;
            overflow: hidden;
            position: relative;
        }

        .visualizer-bar {
            width: 3px;
            margin: 0 1px;
            background-color: #0d6efd;
            position: absolute;
            bottom: 0;
            transition: height 0.1s ease;
        }

        @keyframes pulse {
            0% {
                opacity: 1;
            }

            50% {
                opacity: 0.3;
            }

            100% {
                opacity: 1;
            }
        }

        .timer {
            font-family: monospace;
            font-size: 1.5rem;
        }

        .controls-card {
            background-color: #fff;
            border: 1px solid rgba(0, 0, 0, .125);
            box-shadow: 0 4px 6px rgba(0, 0, 0, .1);
        }

        .audio-stats {
            font-size: 0.875rem;
            color: #6c757d;
        }

        .transcription-box {
            min-height: 100px;
            max-height: 200px;
            overflow-y: auto;
            background-color: #f8f9fa;
            border-radius: 4px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
    </style>
</head>

<body class="bg-light">
    <div class="container py-5">
        <div class="row justify-content-center">
            <div class="col-lg-8">
                <!-- Main Recording Card -->
                <div class="card controls-card mb-4">
                    <div class="card-body">
                        <div class="d-flex align-items-center justify-content-between mb-4">
                            <h2 class="card-title mb-0">
                                <span class="recording-indicator"></span>
                                Audio Recorder
                            </h2>
                            <div class="timer" id="recordingTimer">00:00</div>
                        </div>

                        <!-- Visualizer -->
                        <div class="visualizer-container mb-4" id="visualizer">
                            <!-- Bars will be dynamically added here -->
                        </div>

                        <!-- Audio Stats -->
                        <div class="d-flex justify-content-between audio-stats mb-4">
                            <span>Sample Rate: 44.1kHz</span>
                            <span>Bit Depth: 32-bit</span>
                            <span>Channel: Mono</span>
                        </div>

                        <!-- Controls -->
                        <div class="d-flex gap-2 justify-content-center">
                            <button id="startRecording" class="btn btn-primary btn-lg px-4" onclick="startRecording()">
                                Start Recording
                            </button>
                            <button id="stopRecording" class="btn btn-danger btn-lg px-4" onclick="stopRecording()"
                                disabled>
                                Stop
                            </button>
                        </div>
                    </div>
                </div>

                <!-- Playback Card -->
                <div class="card mb-4" id="recordedAudioContainer" style="display: none;">
                    <div class="card-body">
                        <h5 class="card-title mb-3">Recorded Audio</h5>
                        <audio id="recordedAudioPlayer" controls class="w-100"></audio>
                    </div>
                </div>

                <!-- Transcription Card -->
                <div class="card mb-4">
                    <div class="card-body">
                        <h5 class="card-title" id="transcription-title">Transcription</h5>
                        <div id="transcription" class="transcription-box"></div>
                        <h5 class="card-title" id="transcription-title">Chatgpt Response</h5>
                        <div id="chatgpt_response_box" class="transcription-box"></div>
                    </div>
                </div>

                <!-- <div id="audio_generation_time"></div> -->
                <!-- Query and Generated Audio Section -->
                <!-- <audio id="myAudioElement" controls></audio> -->

                <!-- ChatGPT Response Card -->
                <div class="card" id="chatgpt_audio" style="">
                    <div class="card-body">
                        <div id="audio_generation_time" class="mb-3"></div>
                        <audio id="myAudioElement" controls></audio>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <!-- <script src="static/AudioStreamClient.js"></script> -->
    <script>
        class AudioStreamClient {
            constructor(audioElementId) {
                this.audioElement = document.getElementById(audioElementId);
                if (!this.audioElement) {
                    throw new Error(`Audio element with id "${audioElementId}" not found`);
                }

                this.chunks = [];
                this.currentBlobUrl = null;
                this.isFirstChunk = true;
                this.isPlaying = false;
            }

            appendAudioChunk(chunk) {
                if (!chunk || chunk.byteLength === 0) {
                    console.warn('Received empty chunk');
                    return;
                }

                // Add the new chunk
                this.chunks.push(chunk);

                // Create a new blob from all chunks
                const blob = new Blob(this.chunks, { type: 'audio/mp3' });

                // Create a new URL for the blob
                const blobUrl = URL.createObjectURL(blob);

                // Clean up the previous blob URL
                if (this.currentBlobUrl) {
                    URL.revokeObjectURL(this.currentBlobUrl);
                }

                // Store the current blob URL
                this.currentBlobUrl = blobUrl;

                // Update audio element
                const currentTime = this.audioElement.currentTime;
                this.audioElement.src = blobUrl;

                // If this is the first chunk, start playing immediately
                if (this.isFirstChunk) {
                    this.isFirstChunk = false;
                    this.play();
                } else if (this.isPlaying) {
                    // If already playing, maintain playback
                    this.audioElement.currentTime = currentTime;
                    this.play();
                }
            }

            async play() {
                if (this.currentBlobUrl) {
                    try {
                        await this.audioElement.play();
                        this.isPlaying = true;
                    } catch (error) {
                        console.error('Error playing audio:', error);
                        this.isPlaying = false;
                    }
                }
            }

            pause() {
                this.audioElement.pause();
                this.isPlaying = false;
            }

            reset() {
                this.chunks = [];
                this.isFirstChunk = true;
                this.isPlaying = false;
                if (this.currentBlobUrl) {
                    URL.revokeObjectURL(this.currentBlobUrl);
                    this.currentBlobUrl = null;
                }
                this.audioElement.src = '';
            }

            destroy() {
                this.reset();
                this.audioElement = null;
            }
        }
        let mediaRecorder;
        let audioChunks = [];
        let isRecording = false;
        let transcriptionText = "";
        let websocket;
        let audioContext;
        let processor;
        let input;
        let analyser;
        let visualizerBars = [];
        let timerInterval;
        let startTime;
        let globalStream;
        let isFirstChunkReceived = false;
        let audioClient = new AudioStreamClient("myAudioElement")

        // Initialize WebSocket
        function initWebSocket() {
            if (!websocket || websocket.readyState === WebSocket.CLOSED) {
                websocket = new WebSocket("ws://localhost:8000/ws/audio");
                websocket.onmessage = async function (event) {
                    if (typeof event.data === 'string') {
                        const transcript_data = JSON.parse(event.data);
                        switch (transcript_data.type) {
                            case 'chunk_transcription':
                                console.log("Chunk transcription:", transcript_data);
                                document.getElementById("transcription").innerText += transcript_data.text
                                transcriptionText = document.getElementById("transcription").innerText;
                                break;
                            case 'final_transcription':
                                console.log("Final transcription:", transcript_data.full_text);
                                const transcriptionTitle = document.getElementById("transcription-title")
                                const endTime = Date.now()
                                transcriptionTitle.innerText = "Transcription Time Taken (" + (endTime - startTime) / 1000 + " seconds)"
                                // generateAudio(transcript_data.full_text);
                                break;
                            case 'chatgpt_response':
                                console.log("ChatGPT response:", transcript_data);
                                const chatgptResponse = document.getElementById("chatgpt_response_box")
                                chatgptResponse.innerText = chatgptResponse.innerText + transcript_data.content
                                break
                            case 'audio_end':
                                // audioClient.handleStreamEnd();
                                break
                            default:
                                console.log("Received unknown data:", event.data);
                        }
                    } else if (event.data instanceof Blob) {
                        if (!isFirstChunkReceived) {
                            isFirstChunkReceived = true;
                            endTime = Date.now();
                            const audio_generation_time = document.getElementById("audio_generation_time")
                            audio_generation_time.innerText = "Audio Generation Time Taken (" + (endTime - startTime) / 1000 + " seconds)"
                        }
                        const arrayBuffer = await event.data.arrayBuffer();
                        console.log("Received non-string data:", event.data);
                        audioClient.appendAudioChunk(arrayBuffer);
                    }


                };
                websocket.onclose = function () {
                    console.log("WebSocket closed. Retrying in 2 seconds...");
                    setTimeout(initWebSocket, 2000);
                };
            }
        }

        // Initialize visualizer
        function initVisualizer() {
            const visualizer = document.getElementById('visualizer');
            const barCount = 64;

            for (let i = 0; i < barCount; i++) {
                const bar = document.createElement('div');
                bar.className = 'visualizer-bar';
                bar.style.left = `${(i * 4)}px`;
                visualizerBars.push(bar);
                visualizer.appendChild(bar);
            }
        }

        // Update visualizer with audio data
        function updateVisualizer(audioData) {
            const values = Array.from(audioData);
            const step = Math.floor(values.length / visualizerBars.length);

            for (let i = 0; i < visualizerBars.length; i++) {
                const value = values[i * step];
                const height = (value / 256) * 100;
                visualizerBars[i].style.height = `${height}%`;
            }
        }

        // Update timer display
        function updateTimer() {
            const now = Date.now();
            const diff = now - startTime;
            const seconds = Math.floor(diff / 1000);
            const minutes = Math.floor(seconds / 60);
            const displaySeconds = (seconds % 60).toString().padStart(2, '0');
            const displayMinutes = minutes.toString().padStart(2, '0');
            document.getElementById('recordingTimer').textContent = `${displayMinutes}:${displaySeconds}`;
        }

        function reset_state() {
            isRecording = false;
            audioChunks = [];
            transcriptionText = "";
            document.getElementById("transcription").innerText = "";
            document.getElementById("recordedAudioContainer").style.display = "none";
            document.getElementById("chatgpt_response_box").innerText = "";
            audioClient.destroy();
            audioClient = new AudioStreamClient("myAudioElement")
            isFirstChunkReceived = false
            document.getElementById("audio_generation_time").innerText = "";
            document.body.classList.remove('recording');
            clearInterval(timerInterval);
            document.getElementById('recordingTimer').textContent = '00:00';
            visualizerBars.forEach(bar => {
                bar.style.height = '0%';
            });
        }



        function processAudio(sampleData) {
            // ASR (Automatic Speech Recognition) and VAD (Voice Activity Detection)
            // models typically require mono audio with a sampling rate of 16 kHz,
            // represented as a signed int16 array type.
            //
            // Implementing changes to the sampling rate using JavaScript can reduce
            // computational costs on the server.
            const outputSampleRate = 16000;
            const decreaseResultBuffer = decreaseSampleRate(sampleData, audioContext.sampleRate, outputSampleRate);
            const audioData = convertFloat32ToInt16(decreaseResultBuffer);

            if (websocket && websocket.readyState === WebSocket.OPEN) {
                websocket.send(audioData);
            }
        }

        function decreaseSampleRate(buffer, inputSampleRate, outputSampleRate) {
            if (inputSampleRate < outputSampleRate) {
                console.error("Sample rate too small.");
                return;
            } else if (inputSampleRate === outputSampleRate) {
                return;
            }

            let sampleRateRatio = inputSampleRate / outputSampleRate;
            let newLength = Math.ceil(buffer.length / sampleRateRatio);
            let result = new Float32Array(newLength);
            let offsetResult = 0;
            let offsetBuffer = 0;
            while (offsetResult < result.length) {
                let nextOffsetBuffer = Math.round((offsetResult + 1) * sampleRateRatio);
                let accum = 0, count = 0;
                for (let i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {
                    accum += buffer[i];
                    count++;
                }
                result[offsetResult] = accum / count;
                offsetResult++;
                offsetBuffer = nextOffsetBuffer;
            }
            return result;
        }

        function convertFloat32ToInt16(buffer) {
            let l = buffer.length;
            const buf = new Int16Array(l);
            while (l--) {
                buf[l] = Math.min(1, buffer[l]) * 0x7FFF;
            }
            return buf.buffer;
        }

        function handleMediaRecoder(stream) {
            mediaRecorder = new MediaRecorder(stream, {
                mimeType: 'audio/webm;codecs=opus',
                bitsPerSecond: 256000
            });

            mediaRecorder.ondataavailable = (event) => {
                audioChunks.push(event.data);
            };

            mediaRecorder.onstop = () => {
                const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                const audioUrl = URL.createObjectURL(audioBlob);
                const audioPlayer = document.getElementById('recordedAudioPlayer');
                audioPlayer.src = audioUrl;
                document.getElementById('recordedAudioContainer').style.display = 'block';
                audioPlayer.load();
            };

            mediaRecorder.start();
            isRecording = true;
        }

        async function setupRecordingWorkletNode() {
            await audioContext.audioWorklet.addModule('static/realtime-audio-processor.js');

            return new AudioWorkletNode(
                audioContext,
                'realtime-audio-processor'
            );
        }

        function startRecording() {
            reset_state();
            if (isRecording) return;
            isRecording = true;

            audioContext = new AudioContext();

            let onSuccess = async (stream) => {
                // Push user config to server


                globalStream = stream;
                const input = audioContext.createMediaStreamSource(stream);
                const recordingNode = await setupRecordingWorkletNode();
                handleMediaRecoder(stream);
                recordingNode.port.onmessage = (event) => {
                    processAudio(event.data);
                };
                input.connect(recordingNode);
                document.body.classList.add('recording');
                startTime = Date.now();
                timerInterval = setInterval(updateTimer, 1000);

                analyser = audioContext.createAnalyser();
                analyser.fftSize = 256;
                input.connect(analyser);

                const bufferLength = analyser.frequencyBinCount;
                const dataArray = new Uint8Array(bufferLength);

                function updateAudioLevel() {
                    if (isRecording) {
                        analyser.getByteFrequencyData(dataArray);
                        updateVisualizer(dataArray);
                        requestAnimationFrame(updateAudioLevel);
                    }
                }

                updateAudioLevel();



                document.getElementById("startRecording").disabled = true;
                document.getElementById("stopRecording").disabled = false;
                document.getElementById("generateAudio").disabled = false;
            };
            let onError = (error) => {
                console.error(error);
            };

            navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: true,
                    autoGainControl: false,
                    noiseSuppression: true,
                    latency: 0
                }
            }).then(onSuccess, onError);


        }


        function stopRecording() {
            setTimeout(() => {
                console.log('Stop button clicked');
                if (!isRecording) return;
                isRecording = false;
                mediaRecorder.stop();

                if (processor) {
                    processor.disconnect();
                    processor = null;
                }
                if (audioContext) {
                    audioContext.close().then(() => context = null);
                }

                if (globalStream) {
                    globalStream.getTracks().forEach(track => track.stop());
                }

                if (websocket && websocket.readyState === WebSocket.OPEN) {
                    websocket.send(JSON.stringify({ isFinal: true }));
                    startTime = Date.now();
                }

                document.getElementById("startRecording").disabled = false;
                document.getElementById("stopRecording").disabled = true;
                document.body.classList.remove('recording');
                clearInterval(timerInterval);
            }, 300)
        }

        // Initialize on page load
        initWebSocket();
        initVisualizer();
    </script>
</body>

</html>
